--- main.py
+++ main.py
@@ -1,8 +1,9 @@
 from __future__ import annotations
 
 import json, os, re, time, random, hashlib, logging, sqlite3, threading
-from collections import Counter
+from collections import Counter, deque
+from contextvars import ContextVar
 from typing import Any, Dict, Iterable, Iterator, List, NamedTuple, Optional, Tuple
 from urllib.parse import urlparse
 
@@ -31,19 +32,51 @@
 # ------------------------------------------------------------------------------
 # Pro KOL upgrade switches (all three add-ons)
 # ------------------------------------------------------------------------------
 PRO_KOL_MODE = os.getenv("PRO_KOL_MODE", "0").strip() == "1"
 
 PRO_KOL_POLISH = PRO_KOL_MODE
 PRO_KOL_STRICT = PRO_KOL_MODE
 PRO_KOL_REWRITE = PRO_KOL_MODE
 
 # Rewrite tuning (extra LLM calls; can hit quota)
 PRO_KOL_REWRITE_MAX_TRIES = int(os.getenv("PRO_KOL_REWRITE_MAX_TRIES", "2"))
 PRO_KOL_REWRITE_TEMPERATURE = float(os.getenv("PRO_KOL_REWRITE_TEMPERATURE", "0.7"))
 PRO_KOL_REWRITE_MAX_TOKENS = int(os.getenv("PRO_KOL_REWRITE_MAX_TOKENS", "180"))
 
 # If meme tweet, allow 1 witty line (still no emojis/hashtags)
 PRO_KOL_ALLOW_WIT = os.getenv("PRO_KOL_ALLOW_WIT", "1").strip() != "0"
+
+# Thread + research feature switches
+ENABLE_THREAD_CONTEXT = os.getenv("ENABLE_THREAD_CONTEXT", "1").strip() == "1"
+ENABLE_RESEARCH = os.getenv("ENABLE_RESEARCH", "1").strip() == "1"
+ENABLE_COINGECKO = os.getenv("ENABLE_COINGECKO", "1").strip() == "1"
+COINGECKO_DEMO_KEY = os.getenv("COINGECKO_DEMO_KEY", "").strip()
+RESEARCH_CACHE_TTL_SEC = int(os.getenv("RESEARCH_CACHE_TTL_SEC", "900"))
+
+# Request-scoped context storage (per incoming HTTP request)
+REQUEST_THREAD_CTX: ContextVar[Optional[Dict[str, Any]]] = ContextVar(
+    "REQUEST_THREAD_CTX", default=None
+)
+REQUEST_RESEARCH_CTX: ContextVar[Optional[Dict[str, Any]]] = ContextVar(
+    "REQUEST_RESEARCH_CTX", default=None
+)
+REQUEST_VOICE: ContextVar[Optional[Dict[str, Any]]] = ContextVar(
+    "REQUEST_VOICE", default=None
+)
+
+# In-memory research cache (slug -> data, expires RESEARCH_CACHE_TTL_SEC)
+_RESEARCH_CACHE: Dict[str, tuple[float, Dict[str, Any]]] = {}
+_RESEARCH_CACHE_LOCK = threading.Lock()
+_COINGECKO_DISABLED_UNTIL = 0.0
+
+# Recent voices (roulette anti-repeat)
+RECENT_VOICES = deque(maxlen=5)
+
+# Cached DefiLlama protocol list
+_DEFILLAMA_PROTOCOLS: list[Dict[str, Any]] = []
+_DEFILLAMA_PROTOCOLS_LOADED_AT = 0.0
+
+
+def reset_request_context() -> None:
+    """Clear per-tweet context variables."""
+    REQUEST_THREAD_CTX.set(None)
+    REQUEST_RESEARCH_CTX.set(None)
+    REQUEST_VOICE.set(None)
 
 
 # ------------------------------------------------------------------------------
@@ -106,7 +139,13 @@
             CREATE TABLE IF NOT EXISTS comments_templates_seen(
                 thash TEXT PRIMARY KEY,
                 created_at INTEGER
-            );
+            );
+            CREATE TABLE IF NOT EXISTS entity_map(
+                kind TEXT NOT NULL,
+                k TEXT NOT NULL,
+                slug TEXT NOT NULL,
+                updated_at INTEGER,
+                PRIMARY KEY(kind, k)
+            );
             """
         )
         conn.commit()
@@ -356,6 +395,46 @@
         with get_conn() as c:
             c.execute(
                 "INSERT OR IGNORE INTO comments_templates_seen(thash, created_at) VALUES (?,?)",
                 (thash, now_ts()),
@@ -363,6 +442,46 @@
     except Exception:
         pass
 
+
+def remember_entity(kind: str, key: str, slug: str) -> None:
+    """Persist mapping like ($TICKER|name) -> defillama slug."""
+    try:
+        k_norm = (key or "").strip().lower()
+        if not k_norm or not slug:
+            return
+        with get_conn() as c:
+            c.execute(
+                "INSERT OR REPLACE INTO entity_map(kind, k, slug, updated_at) VALUES (?,?,?,?)",
+                (kind, k_norm, slug, now_ts()),
+            )
+    except Exception:
+        pass
+
+
+def resolve_entity(kind: str, key: str) -> Optional[str]:
+    """Best-effort lookup for previously seen entities."""
+    try:
+        k_norm = (key or "").strip().lower()
+        if not k_norm:
+            return None
+        with get_conn() as c:
+            row = c.execute(
+                "SELECT slug FROM entity_map WHERE kind=? AND k=? LIMIT 1",
+                (kind, k_norm),
+            ).fetchone()
+        if row:
+            return row[0]
+    except Exception:
+        return None
+    return None
+
+
 def _word_trigrams(txt: str) -> set[tuple[str, str, str]]:
     tokens = [t.lower().strip(".,!?") for t in txt.split()]
     return set(zip(tokens, tokens[1:], tokens[2:]))
@@ -772,6 +891,71 @@
     if not tokens:
         return []
     return max(tokens, key=len)
 
+
+# ------------------------------------------------------------------------------
+# Voice roulette (per-request style variation)
+# ------------------------------------------------------------------------------
+VOICE_CARDS: list[Dict[str, Any]] = [
+    {
+        "id": "trader",
+        "label": "Trader",
+        "style": "short, risk/reward framing, entries/exits, sizing, no hype.",
+    },
+    {
+        "id": "builder",
+        "label": "Builder",
+        "style": "talks about shipping, constraints, infra, and real implementation details.",
+    },
+    {
+        "id": "researcher",
+        "label": "Researcher",
+        "style": "dissects mechanisms, token design, incentives, and long-term risks.",
+    },
+    {
+        "id": "skeptic",
+        "label": "Skeptic",
+        "style": "pushes back on hype, asks where the downside is, assumes nothing is free.",
+    },
+    {
+        "id": "curious_friend",
+        "label": "Curious friend",
+        "style": "genuinely trying to understand, asks simple grounded questions.",
+    },
+    {
+        "id": "deadpan_meme",
+        "label": "Deadpan meme",
+        "style": "dry, minimal, slightly absurd but still grounded in the tweet content.",
+    },
+]
+
+
+def pick_voice_card(tweet_text: str, ents: Optional[dict] = None, research: Optional[dict] = None) -> Dict[str, Any]:
+    t = (tweet_text or "").lower()
+    ents = ents or {}
+    research = research or {}
+
+    candidates = VOICE_CARDS
+    weights: list[float] = []
+    for card in candidates:
+        w = 1.0
+        cid = card["id"]
+        if cid == "trader":
+            if (ents.get("cashtags") or "%" in t or "entry" in t or "setup" in t):
+                w += 1.0
+        elif cid == "builder":
+            if any(k in t for k in ("deploy","shipping","testnet","mainnet","contract","dev")):
+                w += 1.0
+        elif cid == "researcher":
+            if any(k in t for k in ("whitepaper","docs","design","mechanism","tokenomics","model")) or research:
+                w += 1.0
+        elif cid == "skeptic":
+            if any(k in t for k in ("rug","scam","ponzi","concern","risk","downside")):
+                w += 1.0
+        elif cid == "curious_friend":
+            if "?" in t or "explain" in t:
+                w += 0.8
+        elif cid == "deadpan_meme":
+            if detect_topic(tweet_text) == "meme":
+                w += 1.0
+        weights.append(w)
+
+    # discourage repeating the exact same voice
+    last = RECENT_VOICES[-1] if RECENT_VOICES else None
+    for i, card in enumerate(candidates):
+        if card["id"] == last and len(candidates) > 1:
+            weights[i] *= 0.3
+
+    total = float(sum(weights) or len(candidates))
+    r = random.random() * total
+    upto = 0.0
+    chosen = candidates[-1]
+    for card, w in zip(candidates, weights):
+        upto += w
+        if r <= upto:
+            chosen = card
+            break
+
+    RECENT_VOICES.append(chosen["id"])
+    return chosen
+
+
 # ------------------------------------------------------------------------------
 # Basic "variety buckets" for two replies (offline path)
 # ------------------------------------------------------------------------------
@@ -1649,11 +1833,15 @@
     """
     out: list[str] = []
 
     for c in candidates:
         c = enforce_word_count_natural(c)
         if not c:
             continue
 
         # Pro strict gate (context-aware)
         if PRO_KOL_STRICT and (not pro_kol_ok(c, tweet_text=tweet_text or "")):
             continue
+
+        # Anti-hallucination check: no new tickers / big numbers
+        if tweet_text and (not anti_hallucination_ok(c, tweet_text)):
+            continue
 
         # Block repeated sentence skeletons (structure-level repetition)
         if template_burned(c):
@@ -1681,16 +1869,60 @@
             out.append(c)
 
     return out
 
 
-def extract_entities(tweet_text: str) -> dict:
-    t = tweet_text or ""
-    cashtags = re.findall(r"\$\w{2,15}", t)
-    handles = re.findall(r"@\w{2,30}", t)
-    decimals = re.findall(r"\d+\.\d+", t)
-    integers = re.findall(r"\b\d+\b", t)
-    return {
-        "cashtags": list(dict.fromkeys(cashtags)),
-        "handles": list(dict.fromkeys(handles)),
-        "numbers": list(dict.fromkeys(decimals + integers)),
-    }
+def extract_entities(tweet_text: str) -> dict:
+    t = tweet_text or ""
+    cashtags = re.findall(r"\$\w{2,15}", t)
+    handles = re.findall(r"@\w{2,30}", t)
+    decimals = re.findall(r"\d+\.\d+", t)
+    integers = re.findall(r"\b\d+\b", t)
+    return {
+        "cashtags": list(dict.fromkeys(cashtags)),
+        "handles": list(dict.fromkeys(handles)),
+        "numbers": list(dict.fromkeys(decimals + integers)),
+    }
+
+
+def anti_hallucination_ok(comment: str, tweet_text: str) -> bool:
+    """Guardrail: no new tickers or big numbers not present in the tweet."""
+    if not tweet_text:
+        return True
+
+    base = extract_entities(tweet_text)
+    cand = extract_entities(comment)
+
+    base_cashtags = set(base.get("cashtags") or [])
+    cand_cashtags = set(cand.get("cashtags") or [])
+    new_cashtags = [c for c in cand_cashtags if c not in base_cashtags]
+    if new_cashtags:
+        return False
+
+    base_numbers = set(base.get("numbers") or [])
+    cand_numbers = cand.get("numbers") or []
+
+    for num in cand_numbers:
+        # ignore small integers 1–10
+        try:
+            val = float(num)
+        except ValueError:
+            continue
+        if val <= 10:
+            continue
+        if num not in base_numbers:
+            return False
+
+    return True
@@ -1869,7 +2101,7 @@
     except Exception:
         return tweet_text
 
 
 def groq_two_comments(tweet_text: str, author: Optional[str], handle: Optional[str], lang: Optional[str], url: str = "") -> list[str]:
@@ -2037,15 +2269,15 @@
     except Exception:
         pass
 
     # Fuse them and re-run uniqueness filter
-    try:
-        sents = split_into_short_sentences(text or "")
-        candidates = enforce_unique(candidates + sents[:2])
-    except Exception:
-        pass
+    try:
+        sents = split_into_short_sentences(text or "")
+        candidates = enforce_unique(candidates + sents[:2], tweet_text=tweet_text)
+    except Exception:
+        pass
 
     # If that still failed, use two offline lines as last resort
-    try:
-        candidates = enforce_unique(candidates + offline_two_comments(tweet_text, author))
-    except Exception:
-        pass
+    try:
+        candidates = enforce_unique(candidates + offline_two_comments(tweet_text, author), tweet_text=tweet_text)
+    except Exception:
+        pass
 
     return candidates[:2] or offline_two_comments(tweet_text, author)
@@ -2059,7 +2291,7 @@
         best = [candidates[0], _rescue_one(tweet_text)]
         extra = offline_two_comments(tweet_text, author)
         if extra:
-            merged = enforce_unique(candidates + extra)
+            merged = enforce_unique(candidates + extra, tweet_text=tweet_text)
             if merged:
                 best = merged[:2]
         return best
@@ -2105,6 +2337,208 @@
         if msg:
             logging.getLogger(__name__).warning("Groq error: %s", msg)
 
 
 # ------------------------------------------------------------------------------
@@ -2128,6 +2462,242 @@
             logging.getLogger(__name__).warning("OpenAI rewrite error: %s", msg)
         return a, b
 
+
+# ------------------------------------------------------------------------------
+# Research plugin (DefiLlama + optional CoinGecko) and prompt context builder
+# ------------------------------------------------------------------------------
+
+def _research_cache_get(key: str) -> Optional[Dict[str, Any]]:
+    now = time.time()
+    try:
+        with _RESEARCH_CACHE_LOCK:
+            item = _RESEARCH_CACHE.get(key)
+            if not item:
+                return None
+            expires_at, data = item
+            if expires_at < now:
+                _RESEARCH_CACHE.pop(key, None)
+                return None
+            return data
+    except Exception:
+        return None
+
+
+def _research_cache_set(key: str, data: Dict[str, Any]) -> None:
+    try:
+        with _RESEARCH_CACHE_LOCK:
+            _RESEARCH_CACHE[key] = (time.time() + RESEARCH_CACHE_TTL_SEC, data)
+            # soft cap
+            if len(_RESEARCH_CACHE) > 128:
+                first = next(iter(_RESEARCH_CACHE))
+                _RESEARCH_CACHE.pop(first, None)
+    except Exception:
+        pass
+
+
+def _coingecko_get(path: str, params: Optional[Dict[str, Any]] = None, timeout: float = 2.0) -> Optional[Dict[str, Any]]:
+    """Best-effort CoinGecko wrapper with 429 circuit breaker."""
+    global _COINGECKO_DISABLED_UNTIL
+    if not ENABLE_COINGECKO:
+        return None
+    if time.time() < _COINGECKO_DISABLED_UNTIL:
+        return None
+
+    base = "https://api.coingecko.com/api/v3"
+    headers: Dict[str, str] = {}
+    if COINGECKO_DEMO_KEY:
+        headers["x-cg-demo-api-key"] = COINGECKO_DEMO_KEY
+
+    try:
+        resp = requests.get(base + path, params=params or {}, headers=headers, timeout=timeout)
+        if resp.status_code == 429:
+            _COINGECKO_DISABLED_UNTIL = time.time() + RESEARCH_CACHE_TTL_SEC
+            return None
+        if not resp.ok:
+            return None
+        data = resp.json()
+        return data if isinstance(data, dict) else None
+    except Exception:
+        return None
+
+
+def _get_defillama_protocols() -> list[Dict[str, Any]]:
+    """Cached DefiLlama protocol list (symbol/name -> slug)."""
+    global _DEFILLAMA_PROTOCOLS, _DEFILLAMA_PROTOCOLS_LOADED_AT
+    now = time.time()
+    if _DEFILLAMA_PROTOCOLS and (now - _DEFILLAMA_PROTOCOLS_LOADED_AT) < RESEARCH_CACHE_TTL_SEC:
+        return _DEFILLAMA_PROTOCOLS
+
+    try:
+        resp = requests.get("https://api.llama.fi/protocols", timeout=2.0)
+        if resp.ok:
+            data = resp.json()
+            if isinstance(data, list):
+                _DEFILLAMA_PROTOCOLS = data
+                _DEFILLAMA_PROTOCOLS_LOADED_AT = now
+                return _DEFILLAMA_PROTOCOLS
+    except Exception:
+        pass
+
+    return _DEFILLAMA_PROTOCOLS or []
+
+
+def _match_protocol_from_keys(keys: list[str]) -> Optional[Dict[str, Any]]:
+    """Rudimentary symbol/name matcher over DefiLlama protocols."""
+    protos = _get_defillama_protocols()
+    if not protos:
+        return None
+
+    norm_keys = [k.strip().lstrip("$").lower() for k in keys if k]
+    best: Optional[Dict[str, Any]] = None
+
+    for proto in protos:
+        try:
+            name = str(proto.get("name") or "").lower()
+            symbol = str(proto.get("symbol") or "").lower()
+            slug = str(proto.get("slug") or "").lower()
+        except Exception:
+            continue
+
+        for k in norm_keys:
+            if not k:
+                continue
+            if k == symbol or k == slug or k in name:
+                best = proto
+                break
+        if best:
+            break
+
+    return best
+
+
+def fetch_research_context(tweet_text: str, ents: Dict[str, Any]) -> Dict[str, Any]:
+    """Resolve project in tweet + fetch DefiLlama snapshot (and optional price)."""
+    if not ENABLE_RESEARCH:
+        return {}
+
+    text = tweet_text or ""
+    ents = ents or {}
+    cashtags: list[str] = ents.get("cashtags") or []
+    tickers = [c.lstrip("$") for c in cashtags]
+    keywords = extract_keywords(text)
+
+    # 1) instant lookup from entity_map
+    slug: Optional[str] = None
+    key_used: Optional[str] = None
+
+    for tkr in tickers:
+        slug = resolve_entity("ticker", tkr)
+        if slug:
+            key_used = f"${tkr}"
+            break
+
+    if not slug:
+        for name in keywords:
+            slug = resolve_entity("name", name)
+            if slug:
+                key_used = name
+                break
+
+    # 2) fallback: DefiLlama protocol list search
+    if not slug:
+        proto = _match_protocol_from_keys(cashtags + keywords)
+        if proto:
+            slug = str(proto.get("slug") or "") or None
+            key_used = proto.get("symbol") or proto.get("name") or key_used
+            if slug:
+                # remember mappings for next time
+                if proto.get("symbol"):
+                    remember_entity("ticker", proto.get("symbol"), slug)
+                if proto.get("name"):
+                    remember_entity("name", proto.get("name"), slug)
+
+    if not slug:
+        return {}
+
+    cache_key = f"defillama:{slug}"
+    cached = _research_cache_get(cache_key)
+    if cached is not None:
+        return cached
+
+    protocol_data: Optional[Dict[str, Any]] = None
+    tvl_data: Any = None
+
+    try:
+        resp = requests.get(f"https://api.llama.fi/protocol/{slug}", timeout=2.0)
+        if resp.ok:
+            maybe = resp.json()
+            if isinstance(maybe, dict):
+                protocol_data = maybe
+    except Exception:
+        protocol_data = None
+
+    try:
+        resp = requests.get(f"https://api.llama.fi/tvl/{slug}", timeout=2.0)
+        if resp.ok:
+            tvl_data = resp.json()
+    except Exception:
+        tvl_data = None
+
+    project_summary: Dict[str, Any] = {}
+    if isinstance(protocol_data, dict):
+        project_summary["name"] = protocol_data.get("name")
+        project_summary["symbol"] = protocol_data.get("symbol")
+        project_summary["category"] = protocol_data.get("category") or protocol_data.get("categorySlug")
+        chains = protocol_data.get("chains") or protocol_data.get("chain")
+        if isinstance(chains, list):
+            project_summary["chains"] = chains
+        elif isinstance(chains, str):
+            project_summary["chains"] = [chains]
+
+    if isinstance(tvl_data, list) and tvl_data:
+        last = tvl_data[-1]
+        try:
+            tvl_val = last.get("totalLiquidityUSD") or last.get("tvl") or last.get("totalLiquidity")
+        except Exception:
+            tvl_val = None
+        project_summary["tvl_usd"] = tvl_val
+        project_summary["tvl_timestamp"] = last.get("date") or last.get("timestamp")
+
+    result: Dict[str, Any] = {
+        "slug": slug,
+        "identified_by": key_used,
+        "project": project_summary,
+        "defillama_raw": {"protocol": protocol_data},
+    }
+
+    # Optional CoinGecko price (best effort, may often be empty)
+    if tickers:
+        cg = _coingecko_get(
+            "/simple/price",
+            params={"ids": tickers[0].lower(), "vs_currencies": "usd"},
+        )
+        if isinstance(cg, dict):
+            result["coingecko_price"] = cg
+
+    _research_cache_set(cache_key, result)
+    return result
+
+
+def fetch_thread_context_vxtwitter(url: str) -> Dict[str, Any]:
+    """Best-effort quote/parent ingestion from a tweet URL via VXTwitter.
+
+    This is intentionally tolerant of schema changes — it just looks for any
+    nested objects that seem like "quoted" or "parent/reply" tweets.
+    """
+    if not ENABLE_THREAD_CONTEXT:
+        return {}
+
+    try:
+        parsed = urlparse(url)
+        parts = [p for p in parsed.path.split("/") if p]
+        tweet_id = next((p for p in reversed(parts) if p.isdigit()), None)
+        if not tweet_id:
+            return {}
+
+        api_url = f"https://api.vxtwitter.com/Twitter/status/{tweet_id}"
+        resp = requests.get(api_url, timeout=2.0)
+        if not resp.ok:
+            return {}
+        data = resp.json() or {}
+    except Exception:
+        return {}
+
+    ctx: Dict[str, Any] = {}
+
+    def _extract_text(obj: Any) -> Optional[str]:
+        if isinstance(obj, dict):
+            for k in ("text", "full_text", "body"):
+                v = obj.get(k)
+                if isinstance(v, str) and v.strip():
+                    return v
+        return None
+
+    # Common fields used by VXTwitter/FxTwitter-style APIs
+    if isinstance(data.get("quoted_tweet"), dict):
+        qtxt = _extract_text(data.get("quoted_tweet"))
+        if qtxt:
+            ctx["quoted"] = qtxt
+    if isinstance(data.get("parent"), dict):
+        ptxt = _extract_text(data.get("parent"))
+        if ptxt:
+            ctx["parent"] = ptxt
+
+    # Generic fallback: scan for likely quote/parent objects
+    if "quoted" not in ctx:
+        for k, v in data.items():
+            if "quote" in k.lower() and isinstance(v, dict):
+                qtxt = _extract_text(v)
+                if qtxt:
+                    ctx["quoted"] = qtxt
+                    break
+
+    if "parent" not in ctx:
+        for k, v in data.items():
+            low = k.lower()
+            if any(token in low for token in ("reply", "parent")) and isinstance(v, dict):
+                ptxt = _extract_text(v)
+                if ptxt:
+                    ctx["parent"] = ptxt
+                    break
+
+    return ctx
+
+
+def build_prompt_context_block() -> str:
+    """Compose a short, safe context block for LLM prompts."""
+    parts: list[str] = []
+
+    thread_ctx = REQUEST_THREAD_CTX.get()
+    if thread_ctx:
+        qt = thread_ctx.get("quoted") or thread_ctx.get("quoted_text")
+        pt = thread_ctx.get("parent") or thread_ctx.get("parent_text")
+        if qt:
+            parts.append(f"Quoted tweet: {qt}")
+        if pt:
+            parts.append(f"Parent tweet: {pt}")
+
+    research_ctx = REQUEST_RESEARCH_CTX.get()
+    if research_ctx:
+        proj = research_ctx.get("project") or {}
+        slug = research_ctx.get("slug") or ""
+        name = (proj.get("name") or "").strip()
+        symbol = (proj.get("symbol") or "").strip()
+        bits: list[str] = []
+        if name:
+            bits.append(name)
+        if symbol:
+            bits.append(f"({symbol})")
+        if slug and slug not in {name, symbol}:
+            bits.append(f"slug: {slug}")
+        tvl = proj.get("tvl_usd")
+        if tvl is not None:
+            try:
+                tvl_val = float(tvl)
+                if tvl_val >= 1e9:
+                    tvl_str = f"{tvl_val/1e9:.1f}B"
+                elif tvl_val >= 1e6:
+                    tvl_str = f"{tvl_val/1e6:.1f}M"
+                else:
+                    tvl_str = f"{tvl_val:,.0f}"
+            except Exception:
+                tvl_str = str(tvl)
+            bits.append(f"TVL ≈ {tvl_str} USD (rough)")
+        category = proj.get("category")
+        if category:
+            bits.append(f"category: {category}")
+        chains = proj.get("chains") or []
+        if isinstance(chains, list) and chains:
+            bits.append("chains: " + ", ".join(chains[:4]))
+        if bits:
+            parts.append("Research snapshot: " + ", ".join(bits))
+
+    voice = REQUEST_VOICE.get()
+    if voice:
+        label = voice.get("label") or voice.get("id")
+        style = voice.get("style") or ""
+        parts.append(f"Voice profile: {label} — {style}")
+
+    if not parts:
+        return ""
+
+    return (
+        "\n\nAdditional context (may be incomplete; never invent facts beyond this):\n"
+        + "\n".join(f"- {p}" for p in parts)
+        + "\n"
+    )
@@ -2240,7 +2810,9 @@
         "- No emojis, no hashtags, no links.\n"
         "- Never reference 'as an AI' or similar.\n"
         "- Avoid generic 'great project' praise; be specific.\n"
         "- Do not make price predictions or promises.\n"
         "- Preserve numbers and tickers exactly (e.g., 17.99 stays 17.99, $SOL stays $SOL).\n"
+        "- If context is unclear or missing, prefer sharp questions over confident claims.\n"
         "\n"
         "Human style:\n"
         "- Sound like a real person on Crypto Twitter.\n"
@@ -2270,8 +2842,15 @@
         "Return JSON:\n"
         "{\n"
         '  "comment_1": "....",\n'
         '  "comment_2": "...."\n'
         "}\n"
-    )
-    mode_line = (mode_line or "").strip()
-    if mode_line:
-        base += "\n" + mode_line + "\n"
-    return base
+    )
+    mode_line = (mode_line or "").strip()
+    if mode_line:
+        base += "\n" + mode_line + "\n"
+
+    # Inject request-scoped context (thread + research + voice) if available
+    ctx_block = build_prompt_context_block()
+    if ctx_block:
+        base += ctx_block
+
+    return base
@@ -2488,40 +3067,43 @@
         return a, b
 
 
 def _rewrite_sys_prompt(topic: str, sentiment: str) -> str:
-    witty = (topic == "meme" and PRO_KOL_ALLOW_WIT)
-    return (
-        "You rewrite or regenerate two tweet replies.\n"
-        "\n"
-        "Hard rules:\n"
-        "- Output exactly 2 comments.\n"
-        "- Each comment must be 6–13 tokens.\n"
-        "- One thought per comment (no second clause like 'thanks for sharing').\n"
-        "- No emojis, no hashtags, no links.\n"
-        "- Do NOT invent facts not present in the tweet.\n"
-        "- Preserve numbers and tickers exactly (17.99 stays 17.99, $SOL stays $SOL).\n"
-        "\n"
-        "Human quality:\n"
-        "- Sound like a real person on CT: grounded, specific, slightly opinionated.\n"
-        "- Avoid hype/fanboy language and generic praise.\n"
-        "- Avoid these phrases: wow, exciting, huge, insane, amazing, awesome, love this, can't wait, sounds interesting.\n"
-        "- If the tweet is funny, allow ONE witty/deadpan line.\n"
-        "\n"
-        "Variety rules:\n"
-        "- Comment #1: observation/claim.\n"
-        "- Comment #2: sharp question OR risk/constraint note.\n"
-        "- Do not reuse the same sentence skeleton (avoid template feel).\n"
-        "- Do not start both comments with the same first word.\n"
-        "\n"
-        f"Context: topic={topic}, sentiment={sentiment}, witty_allowed={str(witty).lower()}.\n"
-        "\n"
-        "Return a JSON array of two strings: [\"...\", \"...\"].\n"
-    )
+    witty = (topic == "meme" and PRO_KOL_ALLOW_WIT)
+    base = (
+        "You rewrite or regenerate two tweet replies.\n"
+        "\n"
+        "Hard rules:\n"
+        "- Output exactly 2 comments.\n"
+        "- Each comment must be 6–13 tokens.\n"
+        "- One thought per comment (no second clause like 'thanks for sharing').\n"
+        "- No emojis, no hashtags, no links.\n"
+        "- Do NOT invent facts not present in the tweet.\n"
+        "- Preserve numbers and tickers exactly (17.99 stays 17.99, $SOL stays $SOL).\n"
+        "\n"
+        "Human quality:\n"
+        "- Sound like a real person on CT: grounded, specific, slightly opinionated.\n"
+        "- Avoid hype/fanboy language and generic praise.\n"
+        "- Avoid these phrases: wow, exciting, huge, insane, amazing, awesome, love this, can't wait, sounds interesting.\n"
+        "- If the tweet is funny, allow ONE witty/deadpan line.\n"
+        "\n"
+        "Variety rules:\n"
+        "- Comment #1: observation/claim.\n"
+        "- Comment #2: sharp question OR risk/constraint note.\n"
+        "- Do not reuse the same sentence skeleton (avoid template feel).\n"
+        "- Do not start both comments with the same first word.\n"
+        "\n"
+        f"Context: topic={topic}, sentiment={sentiment}, witty_allowed={str(witty).lower()}.\n"
+        "\n"
+        "Return a JSON array of two strings: [\"...\", \"...\"].\n"
+    )
+
+    ctx_block = build_prompt_context_block()
+    if ctx_block:
+        base += ctx_block
+
+    return base
@@ -2724,21 +3306,21 @@
         ):
             candidates.extend(gemini_two_comments(tweet_text, author, handle, lang, url=url))
 
         # Extra offline generation to mix in more templates
         if tweet_text and len(candidates) < 4:
             more = offline_two_comments(tweet_text, author)
             if more:
-                candidates = enforce_unique(candidates + more)
+                candidates = enforce_unique(candidates + more, tweet_text=tweet_text)
 
         # If we still only have one candidate, try re-running OpenAI once
         if tweet_text and len(candidates) == 1 and provider_config.get("openai", True):
             try:
                 extra = openai_two_comments(tweet_text, author, handle, lang, url=url)
             except Exception:
                 extra = []
             if extra and len(extra) == 2:
-                candidates = enforce_unique(candidates + extra)
+                candidates = enforce_unique(candidates + extra, tweet_text=tweet_text)
 
     # If everything failed or is disabled, hard fallback to offline
     if len(candidates) < 2:
@@ -2747,11 +3329,11 @@
             try:
                 offline = offline_two_comments(tweet_text, author)
             except Exception:
                 offline = []
             if offline:
-                candidates = enforce_unique(candidates + offline)
+                candidates = enforce_unique(candidates + offline, tweet_text=tweet_text)
 
     # If still nothing, hard fallback to 2 simple offline lines
     if not candidates:
         raw = _rescue_two(tweet_text)
-        candidates = enforce_unique(raw) or raw
+        candidates = enforce_unique(raw, tweet_text=tweet_text) or raw
 
@@ -2759,7 +3341,12 @@
     if not candidates:
         candidates = offline_two_comments(tweet_text, author)
 
-    # Limit to exactly 2 text comments
-    candidates = [c for c in candidates if c][:2]
+    # Limit to exactly 2 text comments
+    candidates = [c for c in candidates if c][:2]
+
+    # Restore decimals and tickers ($TICKER) exactly as in the tweet
+    candidates = [restore_decimals_and_tickers(c, tweet_text) for c in candidates]
@@ -2805,6 +3392,7 @@
     text = sanitize_comment(comment)
     if not text:
         return ""
 
     txt = text
@@ -2821,19 +3409,28 @@
             and word not in {"?", "??", "???"}
         ]
     )
 
     low = out.lower()
     if any(b in low for b in AI_BLOCKLIST) or contains_generic_phrase(low):
         out = " ".join(
             t for t in out.split()
             if t.lower() not in {"honestly", "tbh", "still", "though"}
         ) or out
         out = out.strip()
 
-    out = _ensure_question_punctuation(out)
-
-    if PRO_KOL_POLISH:
-        out = pro_kol_polish(out, topic=detect_topic(raw))
-
-    return out
+    out = _ensure_question_punctuation(out)
+
+    if PRO_KOL_POLISH:
+        out = pro_kol_polish(out, topic=detect_topic(raw))
+
+    return out
@@ -2878,7 +3475,7 @@
             app.logger.error("Error generating comments: %s", exc, exc_info=True)
             return jsonify({
                 "error": "Internal error",
                 "comments": [],
                 "code": "internal_error",
@@ -2906,20 +3503,39 @@
             cleaned.append(url)
 
         results: list[dict] = []
         per_url_metadata: dict[str, dict] = {}
 
         for url in cleaned:
             try:
-                t = fetch_tweet_data(url)
-
+                # Reset per-tweet context vars
+                reset_request_context()
+
+                t = fetch_tweet_data(url)
+
+                # Thread context (quote / parent tweets)
+                thread_ctx: Dict[str, Any] = fetch_thread_context_vxtwitter(url) if ENABLE_THREAD_CONTEXT else {}
+                if thread_ctx:
+                    REQUEST_THREAD_CTX.set(thread_ctx)
+
+                # Lightweight research snapshot (DefiLlama + optional CoinGecko)
+                ents = extract_entities(t.text or "")
+                research_ctx: Dict[str, Any] = fetch_research_context(t.text or "", ents) if ENABLE_RESEARCH else {}
+                if research_ctx:
+                    REQUEST_RESEARCH_CTX.set(research_ctx)
+
+                # Voice roulette for this tweet
+                voice = pick_voice_card(t.text or "", ents, research_ctx)
+                REQUEST_VOICE.set(voice)
+
                 # Prefer handle from upstream payload, fall back to URL parsing
                 handle = t.handle or _extract_handle_from_url(url)
 
                 two = generate_two_comments_with_providers(
                     t.text,
                     t.author_name or None,
                     handle,
                     t.lang or None,
                     url=url,
@@ -2979,19 +3595,38 @@
             return jsonify({
                 "error": "Missing 'url' field",
                 "comments": [],
                 "code": "bad_request",
             }), 400
 
-        t = fetch_tweet_data(url)
-        handle = t.handle or _extract_handle_from_url(url)
+        # Reset per-tweet context vars
+        reset_request_context()
+
+        t = fetch_tweet_data(url)
+
+        # Thread context (quote / parent tweets)
+        thread_ctx: Dict[str, Any] = fetch_thread_context_vxtwitter(url) if ENABLE_THREAD_CONTEXT else {}
+        if thread_ctx:
+            REQUEST_THREAD_CTX.set(thread_ctx)
+
+        ents = extract_entities(t.text or "")
+        research_ctx: Dict[str, Any] = fetch_research_context(t.text or "", ents) if ENABLE_RESEARCH else {}
+        if research_ctx:
+            REQUEST_RESEARCH_CTX.set(research_ctx)
+
+        voice = pick_voice_card(t.text or "", ents, research_ctx)
+        REQUEST_VOICE.set(voice)
+
+        handle = t.handle or _extract_handle_from_url(url)
 
         two = generate_two_comments_with_providers(
             t.text,
             t.author_name or None,
             handle,
             t.lang or None,
             url=url,
         )